{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2a7f12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JD length: 1799\n",
      "Total resumes extracted: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "\n",
    "data_folder = \"E:\\\\Resume_Macher_Project\\\\data\"\n",
    "jd_pdf_path = os.path.join(data_folder, \"jd.pdf\")\n",
    "with pdfplumber.open(jd_pdf_path) as pdf:\n",
    "    text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "\n",
    "resume_text = {}\n",
    "for file in os.listdir(data_folder):\n",
    "    if file.endswith(\".pdf\") and file != \"jd.pdf\":\n",
    "        path = os.path.join(data_folder, file)\n",
    "        resume_text[file] = \"\" \n",
    "        with pdfplumber.open(path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                resume_text[file] += page.extract_text() + \"\\n\"\n",
    "print(\"JD length:\", len(text))\n",
    "print(\"Total resumes extracted:\", len(resume_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed049f",
   "metadata": {},
   "source": [
    "//Data extracted from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70d21be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JD and resumes have been extracted and saved to the output folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "ouput_folder = \"E:\\\\Resume_Macher_Project\\\\output\"\n",
    "os.makedirs(ouput_folder, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(ouput_folder, \"jd.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text.strip())\n",
    "    \n",
    "for filename, text in resume_text.items():\n",
    "    with open(os.path.join(ouput_folder, filename.replace(\".pdf\", \".txt\")), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text.strip())\n",
    "print(\"JD and resumes have been extracted and saved to the output folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0cea5",
   "metadata": {},
   "source": [
    "//Extracted text converted and saved as txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2756905",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"E:\\\\Resume_Macher_Project\\\\output\"\n",
    "output_folder = \"E:\\\\Resume_Macher_Project\\\\output\\\\cleaned\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e6e6914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    doc = nlp(text)\n",
    "    cleaned = \" \".join([token.text for token in doc if not token.is_stop])\n",
    "    return cleaned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18860756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning completed. Cleaned files are saved in the cleaned folder.\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(input_folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read()\n",
    "        cleaned = clean_text(raw_text)\n",
    "        with open(os.path.join(output_folder, file), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(cleaned)\n",
    "print(\"Text cleaning completed. Cleaned files are saved in the cleaned folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00aac7",
   "metadata": {},
   "source": [
    "//txt file cleaned and saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99e91d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('BAAI/bge-base-en')\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcbe7900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 9 cleaned text files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "input_folder = \"E:\\\\Resume_Macher_Project\\\\output\\\\cleaned\"\n",
    "text = {}\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(input_folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            text[file] = f.read()\n",
    "print(\"Loaded:\", len(text),\"cleaned text files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c113f4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n"
     ]
    }
   ],
   "source": [
    "embeddings = {}\n",
    "for filename, content in text.items():\n",
    "    embeddings[filename] = model.encode(content, convert_to_tensor=True)\n",
    "    print(\"embeddings created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a3672f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Resume Matches:\n",
      "Resume8.txt:91.72%match\n",
      "Resume2.txt:89.19%match\n",
      "Resume4.txt:87.16%match\n",
      "Resume1.txt:83.99%match\n",
      "Resume7.txt:83.02%match\n",
      "Resume6.txt:82.55%match\n",
      "Resume3.txt:81.15%match\n",
      "Resume5.txt:76.48%match\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "jd_file = \"jd.txt\"\n",
    "jd_embedding = embeddings[jd_file]\n",
    "similarity_scores = {}\n",
    "for file, emb in embeddings.items():\n",
    "    if file != jd_file:\n",
    "        score = util.pytorch_cos_sim(jd_embedding, emb).item()\n",
    "        similarity_scores[file] = round(score * 100, 2)\n",
    "sorted_scores = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top Resume Matches:\")\n",
    "for file, score in sorted_scores:\n",
    "    print(f\"{file}:{score}%match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43d2cb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kaush_kw3h\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kaush_kw3h\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def extract_keywords_from_jd(jd_text):\n",
    "    jd_text = re.sub(r'[^a-zA-Z0-9 ]', ' ', jd_text) \n",
    "    words = word_tokenize(jd_text.lower())\n",
    "    filtered = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    stemmed = [stemmer.stem(word) for word in filtered]\n",
    "    unique_keywords = list(set(filtered))\n",
    "    lines = jd_text.lower().split('\\n')\n",
    "    jd_title = \"\"\n",
    "    jd_experience = []\n",
    "\n",
    "    for line in lines:\n",
    "        if \"title\" in line or \"job title\" in line:\n",
    "            jd_title = line.strip()\n",
    "        if \"experience\" in line:\n",
    "            jd_experience.append(line.strip())\n",
    "\n",
    "    return jd_title, unique_keywords, jd_experience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9275eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rapidfuzz import fuzz\n",
    "from sentence_transformers import util\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "def match_resumes(jd_text, resume_texts, model, weight_semantics=0.3, weight_keyword=0.7):\n",
    "    jd_title, jd_skills, jd_experience = extract_keywords_from_jd(jd_text)\n",
    "    \n",
    "    jd_embedding = model.encode(jd_text, convert_to_tensor=True)\n",
    "    jd_skills_stemmed = [stemmer.stem(word.lower()) for word in jd_skills]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for filename, text in resume_texts.items():\n",
    "        resume_embedding = model.encode(text, convert_to_tensor=True)\n",
    "        semantic_score = util.pytorch_cos_sim(jd_embedding, resume_embedding).item() * 100\n",
    "\n",
    "        resume_words = preprocess_text(text)\n",
    "        resume_stemmed = [stemmer.stem(word) for word in resume_words]\n",
    "        resume_text_lower = text.lower()\n",
    "\n",
    "        matched_keywords = 0\n",
    "\n",
    "        for kw, stemmed_kw in zip(jd_skills, jd_skills_stemmed):\n",
    "            if kw.lower() in resume_text_lower:\n",
    "                matched_keywords += 1\n",
    "                continue\n",
    "            if stemmed_kw in resume_stemmed:\n",
    "                matched_keywords += 1\n",
    "                continue\n",
    "            for word in resume_words:\n",
    "                if fuzz.partial_ratio(kw.lower(), word) > 90:\n",
    "                    matched_keywords += 1\n",
    "                    break\n",
    "\n",
    "        keyword_score = (matched_keywords / len(jd_skills)) * 100 if jd_skills else 0\n",
    "        final_score = weight_semantics * semantic_score + weight_keyword * keyword_score\n",
    "\n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"semantic_score\": round(semantic_score, 2),\n",
    "            \"keyword_score\": round(keyword_score, 2),\n",
    "            \"final_score\": round(final_score, 2)\n",
    "        })\n",
    "\n",
    "    return sorted(results, key=lambda x: x[\"final_score\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a1fbf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kaush_kw3h\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'Resume2.txt', 'semantic_score': 89.19, 'keyword_score': 94.57, 'final_score': 92.96}\n",
      "{'filename': 'Resume7.txt', 'semantic_score': 83.02, 'keyword_score': 93.02, 'final_score': 90.02}\n",
      "{'filename': 'Resume4.txt', 'semantic_score': 87.16, 'keyword_score': 88.37, 'final_score': 88.01}\n",
      "{'filename': 'Resume3.txt', 'semantic_score': 81.15, 'keyword_score': 75.19, 'final_score': 76.98}\n",
      "{'filename': 'Resume8.txt', 'semantic_score': 91.72, 'keyword_score': 65.12, 'final_score': 73.1}\n",
      "{'filename': 'Resume1.txt', 'semantic_score': 83.99, 'keyword_score': 49.61, 'final_score': 59.93}\n",
      "{'filename': 'Resume6.txt', 'semantic_score': 82.55, 'keyword_score': 47.29, 'final_score': 57.87}\n",
      "{'filename': 'Resume5.txt', 'semantic_score': 76.48, 'keyword_score': 0.0, 'final_score': 22.95}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "jd_text = text[\"jd.txt\"]\n",
    "resumes = {k: v for k, v in text.items() if k != \"jd.txt\"}\n",
    "\n",
    "results = match_resumes(jd_text, resumes, model)\n",
    "for r in results:\n",
    "    print(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6fe394d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to matching_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(\"matching_results.csv\", index=False)\n",
    "print(\"Results saved to matching_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a2431f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'Resume2.txt', 'semantic_score': 89.19, 'keyword_score': 94.57, 'final_score': 92.96}\n",
      "{'filename': 'Resume7.txt', 'semantic_score': 83.02, 'keyword_score': 93.02, 'final_score': 90.02}\n"
     ]
    }
   ],
   "source": [
    "top_matches = [r for r in results if r[\"final_score\"] >= 90]\n",
    "for r in top_matches:\n",
    "    print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
