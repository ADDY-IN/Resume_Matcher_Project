{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a7f12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JD length: 2395\n",
      "Total resumes extracted: 7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "\n",
    "data_folder = \"E:\\\\Resume_Macher_Project\\\\data\"\n",
    "jd_pdf_path = os.path.join(data_folder, \"jd.pdf\")\n",
    "with pdfplumber.open(jd_pdf_path) as pdf:\n",
    "    text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "\n",
    "resume_text = {}\n",
    "for file in os.listdir(data_folder):\n",
    "    if file.endswith(\".pdf\") and file != \"jd.pdf\":\n",
    "        path = os.path.join(data_folder, file)\n",
    "        resume_text[file] = \"\" \n",
    "        with pdfplumber.open(path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                resume_text[file] += page.extract_text() + \"\\n\"\n",
    "print(\"JD length:\", len(text))\n",
    "print(\"Total resumes extracted:\", len(resume_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed049f",
   "metadata": {},
   "source": [
    "//Data extracted from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70d21be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JD and resumes have been extracted and saved to the output folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "ouput_folder = \"E:\\\\Resume_Macher_Project\\\\output\"\n",
    "os.makedirs(ouput_folder, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(ouput_folder, \"jd.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text.strip())\n",
    "    \n",
    "for filename, text in resume_text.items():\n",
    "    with open(os.path.join(ouput_folder, filename.replace(\".pdf\", \".txt\")), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text.strip())\n",
    "print(\"JD and resumes have been extracted and saved to the output folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0cea5",
   "metadata": {},
   "source": [
    "//Extracted text converted and saved as txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2756905",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"E:\\\\Resume_Macher_Project\\\\output\"\n",
    "output_folder = \"E:\\\\Resume_Macher_Project\\\\output\\\\cleaned\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e6e6914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    doc = nlp(text)\n",
    "    cleaned = \" \".join([token.text for token in doc if not token.is_stop])\n",
    "    return cleaned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18860756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning completed. Cleaned files are saved in the cleaned folder.\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(input_folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read()\n",
    "        cleaned = clean_text(raw_text)\n",
    "        with open(os.path.join(output_folder, file), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(cleaned)\n",
    "print(\"Text cleaning completed. Cleaned files are saved in the cleaned folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00aac7",
   "metadata": {},
   "source": [
    "//txt file cleaned and saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99e91d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Resume_Macher_Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('BAAI/bge-base-en')\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcbe7900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8 cleaned text files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "input_folder = \"E:\\\\Resume_Macher_Project\\\\output\\\\cleaned\"\n",
    "text = {}\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(input_folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            text[file] = f.read()\n",
    "print(\"Loaded:\", len(text),\"cleaned text files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c113f4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n"
     ]
    }
   ],
   "source": [
    "embeddings = {}\n",
    "for filename, content in text.items():\n",
    "    embeddings[filename] = model.encode(content, convert_to_tensor=True)\n",
    "    print(\"embeddings created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a3672f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Resume Matches:\n",
      "Resume1.txt:91.98%match\n",
      "Resume2.txt:86.09%match\n",
      "Resume7.txt:84.36%match\n",
      "Resume4.txt:83.74%match\n",
      "Resume6.txt:83.17%match\n",
      "Resume3.txt:78.96%match\n",
      "Resume5.txt:75.93%match\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "jd_file = \"jd.txt\"\n",
    "jd_embedding = embeddings[jd_file]\n",
    "similarity_scores = {}\n",
    "for file, emb in embeddings.items():\n",
    "    if file != jd_file:\n",
    "        score = util.pytorch_cos_sim(jd_embedding, emb).item()\n",
    "        similarity_scores[file] = round(score * 100, 2)\n",
    "sorted_scores = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top Resume Matches:\")\n",
    "for file, score in sorted_scores:\n",
    "    print(f\"{file}:{score}%match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43d2cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "from collections import Counter\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def extract_keywords_from_jd(jd_text, top_k=10):\n",
    "    doc = nlp(jd_text.lower())\n",
    "    keywords = [token.text for token in doc\n",
    "                if token.pos_ in [\"NOUN\", \"PROPN\"]\n",
    "                and len(token.text) > 2\n",
    "                and not token.is_stop\n",
    "                and not token.is_punct]\n",
    "    cleaned = [re.sub(r'[^a-zA-Z0-9\\- ]', '', word) for word in keywords]\n",
    "    freq = Counter(cleaned)\n",
    "    return [word for word, count in freq.most_common(top_k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c3f843d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords: ['deployment', 'models', 'projects', 'data', 'model', 'experience', 'pipelines', 'job', 'engineer', 'hands']\n"
     ]
    }
   ],
   "source": [
    "jd_text = text[\"jd.txt\"] \n",
    "jd_keywords = extract_keywords_from_jd(jd_text, top_k=10)\n",
    "\n",
    "print(\"Extracted Keywords:\", jd_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9275eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "def match_resumes(jd_text, resume_texts, model, weight_semantics=0.4, weight_keyword=0.6):\n",
    "    jd_keywords = extract_keywords_from_jd(jd_text, top_k=10)\n",
    "    jd_embedding = model.encode(jd_text, convert_to_tensor=True)\n",
    "    results = []\n",
    "    for filename, text in resume_texts.items():\n",
    "        resume_embedding = model.encode(text, convert_to_tensor=True)\n",
    "        semantic_score = util.pytorch_cos_sim(jd_embedding, resume_embedding).item() * 100\n",
    "        matched_keywords = sum([1 for kw in jd_keywords if kw.lower() in text.lower()])\n",
    "        keyword_score = (matched_keywords / len(jd_keywords)) * 100\n",
    "\n",
    "        final_score = weight_semantics * semantic_score + weight_keyword * keyword_score\n",
    "\n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"semantic_score\": round(semantic_score, 2),\n",
    "            \"keyword_score\": round(keyword_score, 2),\n",
    "            \"final_score\": round(final_score, 2)\n",
    "        })\n",
    "\n",
    "    return sorted(results, key=lambda x: x[\"final_score\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a1fbf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'Resume1.txt', 'semantic_score': 91.98, 'keyword_score': 80.0, 'final_score': 84.79}\n",
      "{'filename': 'Resume2.txt', 'semantic_score': 86.09, 'keyword_score': 40.0, 'final_score': 58.43}\n",
      "{'filename': 'Resume4.txt', 'semantic_score': 83.74, 'keyword_score': 40.0, 'final_score': 57.49}\n",
      "{'filename': 'Resume7.txt', 'semantic_score': 84.36, 'keyword_score': 30.0, 'final_score': 51.75}\n",
      "{'filename': 'Resume6.txt', 'semantic_score': 83.17, 'keyword_score': 20.0, 'final_score': 45.27}\n",
      "{'filename': 'Resume3.txt', 'semantic_score': 78.96, 'keyword_score': 10.0, 'final_score': 37.58}\n",
      "{'filename': 'Resume5.txt', 'semantic_score': 75.93, 'keyword_score': 0.0, 'final_score': 30.37}\n"
     ]
    }
   ],
   "source": [
    "jd_text = text[\"jd.txt\"]\n",
    "resumes = {k: v for k, v in text.items() if k != \"jd.txt\"}\n",
    "\n",
    "results = match_resumes(jd_text, resumes, model)\n",
    "for r in results:\n",
    "    print(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fe394d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to matching_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(\"matching_results.csv\", index=False)\n",
    "print(\"Results saved to matching_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
