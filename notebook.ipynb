{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a2a7f12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JD length: 1799\n",
      "Total resumes extracted: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "\n",
    "data_folder = \"E:\\\\Resume_Macher_Project\\\\data\"\n",
    "jd_pdf_path = os.path.join(data_folder, \"jd.pdf\")\n",
    "with pdfplumber.open(jd_pdf_path) as pdf:\n",
    "    text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        text += page.extract_text() + \"\\n\"\n",
    "\n",
    "resume_text = {}\n",
    "for file in os.listdir(data_folder):\n",
    "    if file.endswith(\".pdf\") and file != \"jd.pdf\":\n",
    "        path = os.path.join(data_folder, file)\n",
    "        resume_text[file] = \"\" \n",
    "        with pdfplumber.open(path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                resume_text[file] += page.extract_text() + \"\\n\"\n",
    "print(\"JD length:\", len(text))\n",
    "print(\"Total resumes extracted:\", len(resume_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ed049f",
   "metadata": {},
   "source": [
    "//Data extracted from pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "70d21be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JD and resumes have been extracted and saved to the output folder.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "ouput_folder = \"E:\\\\Resume_Macher_Project\\\\output\"\n",
    "os.makedirs(ouput_folder, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(ouput_folder, \"jd.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text.strip())\n",
    "    \n",
    "for filename, text in resume_text.items():\n",
    "    with open(os.path.join(ouput_folder, filename.replace(\".pdf\", \".txt\")), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text.strip())\n",
    "print(\"JD and resumes have been extracted and saved to the output folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0cea5",
   "metadata": {},
   "source": [
    "//Extracted text converted and saved as txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f2756905",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"E:\\\\Resume_Macher_Project\\\\output\"\n",
    "output_folder = \"E:\\\\Resume_Macher_Project\\\\output\\\\cleaned\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3e6e6914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    doc = nlp(text)\n",
    "    cleaned = \" \".join([token.text for token in doc if not token.is_stop])\n",
    "    return cleaned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "18860756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text cleaning completed. Cleaned files are saved in the cleaned folder.\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(input_folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read()\n",
    "        cleaned = clean_text(raw_text)\n",
    "        with open(os.path.join(output_folder, file), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(cleaned)\n",
    "print(\"Text cleaning completed. Cleaned files are saved in the cleaned folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de00aac7",
   "metadata": {},
   "source": [
    "//txt file cleaned and saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99e91d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('BAAI/bge-base-en')\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fcbe7900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 9 cleaned text files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "input_folder = \"E:\\\\Resume_Macher_Project\\\\output\\\\cleaned\"\n",
    "text = {}\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(os.path.join(input_folder, file), \"r\", encoding=\"utf-8\") as f:\n",
    "            text[file] = f.read()\n",
    "print(\"Loaded:\", len(text),\"cleaned text files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c113f4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n",
      "embeddings created\n"
     ]
    }
   ],
   "source": [
    "embeddings = {}\n",
    "for filename, content in text.items():\n",
    "    embeddings[filename] = model.encode(content, convert_to_tensor=True)\n",
    "    print(\"embeddings created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3a3672f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Resume Matches:\n",
      "Resume8.txt:91.72%match\n",
      "Resume2.txt:89.19%match\n",
      "Resume4.txt:87.16%match\n",
      "Resume1.txt:83.99%match\n",
      "Resume7.txt:83.02%match\n",
      "Resume6.txt:82.55%match\n",
      "Resume3.txt:81.15%match\n",
      "Resume5.txt:76.48%match\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "jd_file = \"jd.txt\"\n",
    "jd_embedding = embeddings[jd_file]\n",
    "similarity_scores = {}\n",
    "for file, emb in embeddings.items():\n",
    "    if file != jd_file:\n",
    "        score = util.pytorch_cos_sim(jd_embedding, emb).item()\n",
    "        similarity_scores[file] = round(score * 100, 2)\n",
    "sorted_scores = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top Resume Matches:\")\n",
    "for file, score in sorted_scores:\n",
    "    print(f\"{file}:{score}%match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "43d2cb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched Resumes:\n",
      "\n",
      " Unmatched Resumes:\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "import os\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def extract_keywords_from_jd(jd_text):\n",
    "    job_title = \"\"\n",
    "    skills = []\n",
    "    experience = \"\"  \n",
    "    lines = jd_text.split('\\n')\n",
    "    for line in lines:\n",
    "        lower_line = line.lower()\n",
    "        if 'title' in lower_line and not job_title:\n",
    "            job_title = line.strip()\n",
    "        if 'skills' in lower_line or 'requested' in lower_line:\n",
    "            skills += re.findall(r'\\b\\w+\\b', line.lower())\n",
    "        if 'experience' in lower_line and not experience:\n",
    "            experience = line.strip()\n",
    "    return job_title.lower(), set(skills), experience.lower()\n",
    "\n",
    "with open(os.path.join(input_folder, \"jd.txt\"), \"r\", encoding='utf-8') as f:\n",
    "    jd_raw_text = f.read()\n",
    "\n",
    "jd_title, jd_skills, jd_experience = extract_keywords_from_jd(jd_raw_text)\n",
    "\n",
    "\n",
    "resume_text = {}\n",
    "import os\n",
    "data_folder = \"E:\\\\Resume_Macher_Project\\\\data\"\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.endswith(\".txt\") and filename != \"jd.txt\":\n",
    "        with open(os.path.join(data_folder, filename), 'r', encoding='utf-8') as f:\n",
    "            resume_text[filename] = f.read()\n",
    "\n",
    "\n",
    "matched_resumes = []\n",
    "unmatched_resumes = []\n",
    "\n",
    "for filename, resume in resume_text.items():\n",
    "    text = resume.lower()\n",
    "    skill_match = any(skill in text for skill in jd_skills)\n",
    "    title_match = jd_title.split()[-1] in text if jd_title else False\n",
    "    experience_match = jd_experience.split()[-1] in text if jd_experience else False\n",
    "\n",
    "    if skill_match or title_match or experience_match:\n",
    "        matched_resumes.append((filename, text))\n",
    "    else:\n",
    "        unmatched_resumes.append((filename, text))\n",
    "\n",
    "print(\"Matched Resumes:\")\n",
    "for f in matched_resumes:\n",
    "    print(f[0])\n",
    "\n",
    "print(\"\\n Unmatched Resumes:\")\n",
    "for f in unmatched_resumes:\n",
    "    print(f[0])\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9275eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "def match_resumes(jd_text, resume_texts, model, weight_semantics=0.4, weight_keyword=0.6):\n",
    "    jd_keywords = extract_keywords_from_jd(jd_text)\n",
    "    jd_embedding = model.encode(jd_text, convert_to_tensor=True)\n",
    "    results = []\n",
    "    for filename, text in resume_texts.items():\n",
    "        resume_embedding = model.encode(text, convert_to_tensor=True)\n",
    "        semantic_score = util.pytorch_cos_sim(jd_embedding, resume_embedding).item() * 100\n",
    "        matched_keywords = sum([1 for kw in jd_keywords if kw.lower() in text.lower()])\n",
    "        keyword_score = (matched_keywords / len(jd_keywords)) * 100\n",
    "\n",
    "        final_score = weight_semantics * semantic_score + weight_keyword * keyword_score\n",
    "\n",
    "        results.append({\n",
    "            \"filename\": filename,\n",
    "            \"semantic_score\": round(semantic_score, 2),\n",
    "            \"keyword_score\": round(keyword_score, 2),\n",
    "            \"final_score\": round(final_score, 2)\n",
    "        })\n",
    "\n",
    "    return sorted(results, key=lambda x: x[\"final_score\"], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2a1fbf74",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m jd_text = text[\u001b[33m\"\u001b[39m\u001b[33mjd.txt\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      2\u001b[39m resumes = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m text.items() \u001b[38;5;28;01mif\u001b[39;00m k != \u001b[33m\"\u001b[39m\u001b[33mjd.txt\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m results = \u001b[43mmatch_resumes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjd_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresumes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(r)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mmatch_resumes\u001b[39m\u001b[34m(jd_text, resume_texts, model, weight_semantics, weight_keyword)\u001b[39m\n\u001b[32m      7\u001b[39m resume_embedding = model.encode(text, convert_to_tensor=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m semantic_score = util.pytorch_cos_sim(jd_embedding, resume_embedding).item() * \u001b[32m100\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m matched_keywords = \u001b[38;5;28msum\u001b[39m([\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m kw \u001b[38;5;129;01min\u001b[39;00m jd_keywords \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mkw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m() \u001b[38;5;129;01min\u001b[39;00m text.lower()])\n\u001b[32m     10\u001b[39m keyword_score = (matched_keywords / \u001b[38;5;28mlen\u001b[39m(jd_keywords)) * \u001b[32m100\u001b[39m\n\u001b[32m     12\u001b[39m final_score = weight_semantics * semantic_score + weight_keyword * keyword_score\n",
      "\u001b[31mAttributeError\u001b[39m: 'set' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "jd_text = text[\"jd.txt\"]\n",
    "resumes = {k: v for k, v in text.items() if k != \"jd.txt\"}\n",
    "\n",
    "results = match_resumes(jd_text, resumes, model)\n",
    "for r in results:\n",
    "    print(r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe394d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to matching_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.to_csv(\"matching_results.csv\", index=False)\n",
    "print(\"Results saved to matching_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8febd517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Resumes with final score >= 75%:\n",
      "\n",
      "{'filename': 'Resume8.txt', 'semantic_score': 91.72, 'keyword_score': 70.0, 'final_score': 78.69}\n",
      "{'filename': 'Resume2.txt', 'semantic_score': 89.19, 'keyword_score': 70.0, 'final_score': 77.68}\n"
     ]
    }
   ],
   "source": [
    "results = match_resumes(jd_text, resumes, model)\n",
    "filtered_results = [r for r in results if r[\"final_score\"] >= 75.0]\n",
    "print(\"\\n Resumes with final score >= 75%:\\n\")\n",
    "for r in filtered_results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f35f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved filtered results to filtered_matching_results.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_filtered = pd.DataFrame(filtered_results)\n",
    "df_filtered.to_csv(\"filtered_matching_results.csv\", index=False)\n",
    "print(\"Saved filtered results to filtered_matching_results.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
